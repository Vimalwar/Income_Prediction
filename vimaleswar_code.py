# -*- coding: utf-8 -*-
"""Vimaleswar_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V_gnxvtxKXmoClhLhON-29H90dvjLKkT

**Income Prediction:**

An individual’s annual income results from various factors. The objective is to predict, if the salary is greater than $50K or not.

The dataset contains 15 columns

Target filed: Income

-- The income is divide into two classes: <=50K and >50K

Number of attributes: 14

-- These are the demographics and other features to describe a person

About the Dataset:

1. age: age of a data article
2. workclass: the place where they work
3. fnlwgt: means final weight
4. education: education of a particular person
5. education Num: education number means number of years they educated
6. marital status: Represent the marital status of the individual
7. occupation: Occupation of the individual
8. relationship: Relationship of the individual
9. race: Race of the individual
10. gender: Gender of the individual
11. capital-gain: Captial-gain of individual
12. capital-loss: Capital-loss of individual
13. hours-per-week: How many Hours per week they work
14. native-country: Native country of the individual








           1: Income above >=50k
           0: Income below <50k

## Importing the Dependencies:

We are going to use the Google Collab GPU
"""

import tensorflow as tf

tf.test.gpu_device_name()

#Basic Libraries for Machine Learning
import numpy as np 
import pandas as pd

# For Date Visualization and EDA
import matplotlib.pyplot as plt
import seaborn as sns
import plotly
import plotly.express as px
import plotly.figure_factory as ff
import plotly.graph_objects as go

# Importing sklearn library
import sklearn

from google.colab import files

# For Data Encoding
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder

# For Correlation matrix
from scipy.stats import pearsonr

# For Scaling the datasets
from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import cross_val_score
from sklearn.feature_selection import SelectKBest

# Splitting the datasets
from sklearn.model_selection import train_test_split

# For checking accuracy and confusion matrix
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report

#Cross-validation
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV 
from sklearn.pipeline import make_pipeline

from scipy.stats import uniform, randint

# For importing the models
from sklearn.linear_model import LogisticRegression
from sklearn import svm
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import f1_score, make_scorer

"""# Data Collection and Pre-Processing:"""

# loading the dataset to pandas DataFrame
train = pd.read_csv('/content/train.csv')

# printing the first 5 rows of the dataframe
train.head()

# printing the last 5 rows of the dataframe
train.tail()

# number of rows and columns
train.shape

# loading the train class label dataset to pandas DataFrame
train_class_labels = pd.read_csv('/content/train_class_labels.csv')

# printing the first 5 rows of the dataframe
train_class_labels.head()

# number of rows and columns
train_class_labels.shape

"""Now we are going to merge the two datasets"""

income_dataset = pd.merge(train,train_class_labels)

# printing the first 5 rows of the dataframe
income_dataset.head()

# printing the last 5 rows of the dataframe
income_dataset.tail()

# number of rows and columns
income_dataset.shape

"""Now we are going to remove the first column, as that is just a Serial number."""

# drop the "Unnamed: 0" column from the dataFrame
income_dataset = income_dataset.drop(columns='Unnamed: 0', axis=1)

income_dataset.head()

# number of rows and columns
income_dataset.shape

income_dataset.columns

income_dataset.info()

"""From the above, we can see that there are 7 numerical column and 8 categorical column"""

# statistical measures
income_dataset.describe()

income_dataset.describe().transpose()

# number of missing values in each column
income_dataset.isnull().sum()

"""# Handling the Missing values:

The handling of missing data is very important during the preprocessing of the dataset as many machine learning algorithms do not support missing values. There are many ways of handling the missing/null values.

**1. Deleting Rows with missing values:**

Missing values can be handled by deleting the rows or columns having null values. If columns have more than half of the rows as null then the entire column can be dropped. The rows which are having one or more columns values as null can also be dropped.

***Pros:***

A model trained with the removal of all missing values creates a robust model.

***Cons:***

Loss of a lot of information.

Works poorly if the percentage of missing values is excessive in comparison to the complete dataset.

**2.1.Impute missing values with Mean/Median for Numeric columns:**

Columns in the dataset which are having numeric continuous values can be replaced with the mean, median, of values in the column. This method can prevent the loss of data compared to the earlier method. Replacing the above two approximations (mean, median) is a statistical approach to handle the missing values.

***Pros:***

Prevent data loss which results in deletion of rows or columns.

***Cons:***

Works only with numerical continuous variables.
Can cause data leakage.

**2.2.Imputation method for categorical columns:**

When missing values is from categorical columns (string or numerical) then the missing values can be replaced with the most frequent category/values(called as mode). If the number of missing values is very large then it can be replaced with a new category.

***Pros:***

Prevent data loss which results in deletion of rows or columns.
Works well with a small dataset and is easy to implement.

***Cons:***

Works only with categorical variables.
Addition of new features to the model while encoding, which may result in poor performance.

## Method - 1:
"""

# dropping the missing values
income_dataset1 = income_dataset.dropna()

# number of missing values in each column
income_dataset1.isnull().sum()

# number of rows and columns
income_dataset1.shape

income_dataset1.head(50)   # Here we can able to see that the 33rd row is removed as we have a null value in workclass and occupation column

"""## Method - 2:"""

# finding the mode value of "workclass" column
# finding the highest occurance of a value
print(income_dataset['workclass'].mode())

print(income_dataset['workclass'].mode()[0])

income_dataset['workclass'].fillna(income_dataset['workclass'].mode()[0], inplace=True)

# finding the mode value of "occupation" column
# finding the highest occurance of a value
print(income_dataset['occupation'].mode())

print(income_dataset['occupation'].mode()[0])

income_dataset['occupation'].fillna(income_dataset['occupation'].mode()[0], inplace=True)

# finding the mode value of "native-country" column
# finding the highest occurance of a value
print(income_dataset['native-country'].mode())

print(income_dataset['native-country'].mode()[0])

income_dataset['native-country'].fillna(income_dataset['native-country'].mode()[0], inplace=True)

income_dataset2 = income_dataset

# number of missing values in each column
income_dataset2.isnull().sum()

income_dataset2.shape

income_dataset2.head(50) # Here we can able to see that in 33rd row the workplace column changed from null value to as private
                         # and the occupation column changed from null value to craft-repair

"""So, now we have two datasets 
1. Income_dataset1 where we droped the entire row with null values.
2. Income_dataset2 where we applied the mode method for the categorical columns.

# Data Analysis and Data Visualization:

## For the Categorical Columns:

Categorical Features:
- workclass
- education
- marital-status
- occupation
- relationship
- race
- gender
- native-country
"""

category_col=['workclass','education','marital-status', 'occupation', 'relationship', 'race', 'gender','native-country']
l=[]
for i in category_col:
    print('-------------------')
    print(i)
    print('-------------------')
    print((income_dataset1[i].value_counts()))
    print('------------------------------------------------------------------')

category_feacture=['workclass','education','marital-status', 'occupation', 'relationship', 'race', 'gender','native-country']
l=[]
for i in category_col:
    print('-------------------')
    print(i)
    print('-------------------')
    print((income_dataset2[i].value_counts()))
    print('------------------------------------------------------------------')

#making a countplot for "Workclass" column
plt.figure(figsize=(6,6))
sns.countplot(x='workclass', data=income_dataset1, order = income_dataset1['workclass'].value_counts().index)
plt.xticks(rotation=90);
plt.show

#making a countplot for "Workclass" column
plt.figure(figsize=(6,6))
sns.countplot(x='workclass', data=income_dataset2, order = income_dataset2['workclass'].value_counts().index)
plt.xticks(rotation=90);
plt.show

#making a countplot for "Workclass" with respect to "income_>50k" column
plt.figure(figsize=(6,6))
sns.countplot(x='workclass', data=income_dataset1, order = income_dataset1['workclass'].value_counts().index,
              hue=income_dataset1['income_>50K'])
plt.xticks(rotation=90);
plt.show

#making a countplot for "Workclass" with respect to "income_>50k" column
plt.figure(figsize=(6,6))
sns.countplot(x='workclass', data=income_dataset2, order = income_dataset2['workclass'].value_counts().index,
              hue=income_dataset2['income_>50K'])
plt.xticks(rotation=90);
plt.show

#making a countplot for "Occupation" with respect to "income_>50k" column
plt.figure(figsize=(6,6))
sns.countplot(x='occupation', data=income_dataset1, order = income_dataset1['occupation'].value_counts().index,
              hue=income_dataset1['income_>50K'])
plt.xticks(rotation=90);
plt.show

#making a countplot for "Occupation" with respect to "income_>50k" column
plt.figure(figsize=(6,6))
sns.countplot(x='occupation', data=income_dataset2, order = income_dataset2['occupation'].value_counts().index,
              hue=income_dataset2['income_>50K'])
plt.xticks(rotation=90);
plt.show

#making a countplot for "marital-status" with respect to "income_>50k" column
plt.figure(figsize=(6,6))
sns.countplot(x='marital-status', data=income_dataset1, order = income_dataset1['marital-status'].value_counts().index,
              hue=income_dataset1['income_>50K'])
plt.xticks(rotation=90);
plt.show

#making a countplot for "marital-status" with respect to "income_>50k" column
plt.figure(figsize=(6,6))
sns.countplot(x='marital-status', data=income_dataset2, order = income_dataset2['marital-status'].value_counts().index,
              hue=income_dataset2['income_>50K'])
plt.xticks(rotation=90);
plt.show

#making a countplot for "race" with respect to "income_>50k" column
plt.figure(figsize=(6,6))
sns.countplot(x='race', data=income_dataset1, order = income_dataset1['race'].value_counts().index,
              hue=income_dataset1['income_>50K'])
plt.xticks(rotation=90);
plt.show

#making a countplot for "race" with respect to "income_>50k" column
plt.figure(figsize=(6,6))
sns.countplot(x='race', data=income_dataset2, order = income_dataset2['race'].value_counts().index,
              hue=income_dataset2['income_>50K'])
plt.xticks(rotation=90);
plt.show

#Pie chart
fig = go.Figure(data=[go.Pie(labels=income_dataset1['gender'])])
fig.update_traces(textposition='inside',textinfo='percent+label')
fig.update_layout(title="Pie-chart for Gender Column")
fig.show()

#Pie chart
fig = go.Figure(data=[go.Pie(labels=income_dataset2['gender'])])
fig.update_traces(textposition='inside',textinfo='percent+label')
fig.update_layout(title="Pie-chart for Gender Column")
fig.show()

sorted_counts = income_dataset1['relationship'].value_counts()
plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90,
        counterclock = False, wedgeprops = {'width' : 0.5});
plt.axis('square');

sorted_counts = income_dataset2['relationship'].value_counts()
plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90,
        counterclock = False, wedgeprops = {'width' : 0.5});
plt.axis('square');

"""## For the Numerical Columns:

Numerical Features:
- Age
- fnlwgt
- educational-num
- capital gain
- capital loss
- hours per week
- income_>50k
"""

#making a countplot for "educational-num" column with respect to "Income=>50k" column
plt.figure(figsize=(6,6))
sns.countplot(x='educational-num', data=income_dataset1, order = income_dataset1['educational-num'].value_counts().index,
              hue=income_dataset1['income_>50K'])
plt.show

#making a countplot for "educational-num" column with respect to "Income=>50k" column
plt.figure(figsize=(6,6))
sns.countplot(x='educational-num', data=income_dataset2, order = income_dataset2['educational-num'].value_counts().index,
              hue=income_dataset2['income_>50K'])
plt.show

"""Now for Income_>50K column:"""

plt.hist(income_dataset1['age'], edgecolor='black')
plt.title('Age Histogram')
plt.axvline(np.mean(income_dataset1['age']), color='yellow', label='average age')
plt.legend()

plt.hist(income_dataset2['age'], edgecolor='black')
plt.title('Age Histogram')
plt.axvline(np.mean(income_dataset2['age']), color='yellow', label='average age')
plt.legend()

plt.figure(figsize=(6,6))
sns.countplot(x='income_>50K', data=income_dataset2)
plt.show

income_dataset1['income_>50K'].value_counts()

income_dataset2['income_>50K'].value_counts()

print(f"<= 50k for Income_dataset1 : {round(30635 / 40727 * 100 , 2)}")
print(f"> 50k for Income_dataset1: {round(10092 / 40727 * 100 , 2)}")

print(f"<= 50k for Income_dataset2 : {round(33439 / 43957 * 100 , 2)}")
print(f"> 50k for Income_dataset2: {round(10518 / 43957 * 100 , 2)}")

"""# Data/Label Encoding:

We have 8 Categorical Columns in the dataset.

Computers can understand only numerical data's, they can't understand text data's. So, we have to convert the categorical columns into numerical columns so that computer can understand. This is termed as Data/Label Encoding in Machine Learning.
"""

encoder = LabelEncoder()

income_dataset1['workclass'] = encoder.fit_transform(income_dataset1['workclass'])

income_dataset1['education'] = encoder.fit_transform(income_dataset1['education'])

income_dataset1['marital-status'] = encoder.fit_transform(income_dataset1['marital-status'])

income_dataset1['occupation'] = encoder.fit_transform(income_dataset1['occupation'])

income_dataset1['relationship'] = encoder.fit_transform(income_dataset1['relationship'])

income_dataset1['race'] = encoder.fit_transform(income_dataset1['race'])

income_dataset1['gender'] = encoder.fit_transform(income_dataset1['gender'])

income_dataset1['native-country'] = encoder.fit_transform(income_dataset1['native-country'])

# printing the first 5 rows of the dataframe
income_dataset1.head()

# printing the last 5 rows of the dataframe
income_dataset1.tail()

"""Here for Example 

In Race column: 

- White - 4              
- Black - 2             
- Asian-Pac-Islander - 1  
- Amer-Indian-Eskimo - 0  
- Other - 3

In Gender column:

- Male - 1
- Female - 0

Same for all other columns

The labels are given in the ascending order.
"""

# number of rows and columns
income_dataset1.shape

income_dataset1.info()

"""There is no text data in any columns in Income_dataset1

Same Label Encoding we are doing for income_dataset2
"""

income_dataset2['workclass'] = encoder.fit_transform(income_dataset2['workclass'])

income_dataset2['education'] = encoder.fit_transform(income_dataset2['education'])

income_dataset2['marital-status'] = encoder.fit_transform(income_dataset2['marital-status'])

income_dataset2['occupation'] = encoder.fit_transform(income_dataset2['occupation'])

income_dataset2['relationship'] = encoder.fit_transform(income_dataset2['relationship'])

income_dataset2['race'] = encoder.fit_transform(income_dataset2['race'])

income_dataset2['gender'] = encoder.fit_transform(income_dataset2['gender'])

income_dataset2['native-country'] = encoder.fit_transform(income_dataset2['native-country'])

# printing the first 5 rows of the dataframe
income_dataset2.head()

# printing the last 5 rows of the dataframe
income_dataset2.tail()

# number of rows and columns
income_dataset2.shape

income_dataset2.info()

"""There is no text data in any column in Income_dataset2

# Correlation of Columns:

The most commonly used is the **Pearson Correlation** method.
"""

correlation1 = income_dataset1.corr(method='pearson')

# constructing a heatmap to understand the correlation
plt.figure(figsize=(15, 15))
sns.heatmap(correlation1, cbar=True, square=True, annot=True, fmt='.3f', annot_kws={'size':8}, cmap='Oranges')

correlation2 = income_dataset2.corr(method='pearson')

# constructing a heatmap to understand the correlation
plt.figure(figsize=(15, 15))
sns.heatmap(correlation2, cbar=True, square=True, annot=True, fmt='.3f', annot_kws={'size':8}, cmap='Greens')

"""# Splitting features and target:"""

X_1 = income_dataset1.iloc[:,:-1]
Y_1 = income_dataset1['income_>50K']

print(X_1)   # Here we can see that the Income(Target) column is removed

print(Y_1)   # Here we can see that only Income(Target) column is present

"""Stratified sampling will be adopted in dividing train and test set to preserve the ratio between two classes."""

X_train_1, X_test_1, Y_train_1, Y_test_1 = train_test_split(X_1, Y_1, test_size = 0.2, random_state = 30, stratify = Y_1)

print(X_train_1.shape, X_test_1.shape, Y_train_1.shape, Y_test_1.shape)

X_2 = income_dataset2.iloc[:,:-1]
Y_2 = income_dataset2['income_>50K']

print(X_2)

print(Y_2)

X_train_2, X_test_2, Y_train_2, Y_test_2 = train_test_split(X_2, Y_2, test_size = 0.2, random_state = 30, stratify = Y_2)

print(X_train_2.shape, X_test_2.shape, Y_train_2.shape, Y_test_2.shape)

"""# Feature Scaling:

Feature Scaling is a technique to standardize the independent features present in the data in a fixed range. It is performed to handle highly varying magnitudes or values or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values, higher and consider smaller values as the lower values, regardless of the unit of the values.(done to reduce bias in algorithm)

Here we have used StandardScaler method

**Standardization:** It is a very effective technique which re-scales a feature value so that it has distribution with 0 mean value and variance equals to 1.
"""

scaler = StandardScaler()

X_train_1 = pd.DataFrame(scaler.fit_transform(X_train_1), columns=X_train_1.columns)
X_test_1 = pd.DataFrame(scaler.fit_transform(X_test_1), columns=X_test_1.columns)

X_train_1.head()

X_train_1.tail()

X_test_1.head()

print(X_train_1.shape, X_test_1.shape, Y_train_1.shape, Y_test_1.shape)

X_train_2 = pd.DataFrame(scaler.fit_transform(X_train_2), columns=X_train_2.columns)
X_test_2 = pd.DataFrame(scaler.fit_transform(X_test_2), columns=X_test_2.columns)

X_train_2.head()

X_train_2.tail()

X_test_2.head()

print(X_train_2.shape, X_test_2.shape, Y_train_2.shape, Y_test_2.shape)

"""# Training the data's with different models(Without Hyperparameter tunning):

**1. Logistic Regression Model**
"""

lr = LogisticRegression()
lr.fit(X_train_1, Y_train_1)
prediction_lr_1 = lr.predict(X_test_1)
print('\n**********Logistic Regression**********\n')
acc_lr_1 = accuracy_score(Y_test_1, prediction_lr_1)
con_lr_1 = confusion_matrix(Y_test_1, prediction_lr_1)
class_lr_1 = classification_report(Y_test_1, prediction_lr_1)
print("Accuracy Score: \n", acc_lr_1)
print("Confusion Matrix: \n", con_lr_1)
print("Classification Report: \n", class_lr_1)

lr = LogisticRegression()
lr.fit(X_train_2,Y_train_2)
prediction_lr_2 = lr.predict(X_test_2)
print('\n********** Logistic Regression **********\n')
acc_lr_2 = accuracy_score(Y_test_2, prediction_lr_2)
con_lr_2 = confusion_matrix(Y_test_2, prediction_lr_2)
class_lr_2 = classification_report(Y_test_2, prediction_lr_2)
print("Accuracy Score: \n", acc_lr_2)
print("Confusion Matrix: \n", con_lr_2)
print("Classification Report: \n", class_lr_2)

"""**2. Decision Tree Classifier**"""

dtc = DecisionTreeClassifier()
dtc.fit(X_train_1, Y_train_1)
prediction_dtc_1 = dtc.predict(X_test_1)
print('\n********** Decision Tree Classifier **********\n')
acc_dtc_1 = accuracy_score(Y_test_1, prediction_dtc_1)
con_dtc_1 = confusion_matrix(Y_test_1, prediction_dtc_1)
class_dtc_1 = classification_report(Y_test_1, prediction_dtc_1)
print("Accuracy Score: \n", acc_dtc_1)
print("Confusion Matrix: \n", con_dtc_1)
print("Classification Report: \n", class_dtc_1)

dtc = DecisionTreeClassifier()
dtc.fit(X_train_2, Y_train_2)
prediction_dtc_2 = dtc.predict(X_test_2)
print('\n********** Decision Tree Classifier **********\n')
acc_dtc_2 = accuracy_score(Y_test_2, prediction_dtc_2)
con_dtc_2 = confusion_matrix(Y_test_2, prediction_dtc_2)
class_dtc_2 = classification_report(Y_test_2, prediction_dtc_2)
print("Accuracy Score: \n", acc_dtc_2)
print("Confusion Matrix: \n", con_dtc_2)
print("Classification Report: \n", class_dtc_2)

"""**3. Random Forest Classifier**"""

rfc = RandomForestClassifier()
rfc.fit(X_train_1, Y_train_1)
prediction_rfc_1 = rfc.predict(X_test_1)
print('\n********** Random Forest Classifier **********\n')
acc_rfc_1 = accuracy_score(Y_test_1, prediction_rfc_1)
con_rfc_1 = confusion_matrix(Y_test_1, prediction_rfc_1)
class_rfc_1 = classification_report(Y_test_1, prediction_rfc_1)
print("Accuracy Score: \n", acc_rfc_1)
print("Confusion Matrix: \n", con_rfc_1)
print("Classification Report: \n", class_rfc_1)

rfc = RandomForestClassifier()
rfc.fit(X_train_2, Y_train_2)
prediction_rfc_2 = rfc.predict(X_test_2)
print('\n********** Decision Tree Classifier **********\n')
acc_rfc_2 = accuracy_score(Y_test_2, prediction_rfc_2)
con_rfc_2 = confusion_matrix(Y_test_2, prediction_rfc_2)
class_rfc_2 = classification_report(Y_test_2, prediction_rfc_2)
print("Accuracy Score: \n", acc_rfc_2)
print("Confusion Matrix: \n", con_rfc_2)
print("Classification Report: \n", class_rfc_2)

"""**4. Support Vector Classifier**"""

svm = svm.SVC(kernel = 'linear')
svm.fit(X_train_1, Y_train_1)
prediction_svm_1 = svm.predict(X_test_1)
print('\n********** Support Vector Classifier **********\n')
acc_svm_1 = accuracy_score(Y_test_1, prediction_svm_1)
con_svm_1 = confusion_matrix(Y_test_1, prediction_svm_1)
class_svm_1 = classification_report(Y_test_1, prediction_svm_1)
print("Accuracy Score: \n", acc_svm_1)
print("Confusion Matrix: \n", con_svm_1)
print("Classification Report: \n", class_svm_1)

from sklearn import svm
from sklearn.svm import SVC

svm_2 = svm.SVC(kernel = 'linear')
svm_2.fit(X_train_2, Y_train_2)
prediction_svm_2 = svm_2.predict(X_test_2)
print('\n********** Support Vector Classifier **********\n')
acc_svm_2 = accuracy_score(Y_test_2, prediction_svm_2)
con_svm_2 = confusion_matrix(Y_test_2, prediction_svm_2)
class_svm_2 = classification_report(Y_test_2, prediction_svm_2)
print("Accuracy Score: \n", acc_svm_2)
print("Confusion Matrix: \n", con_svm_2)
print("Classification Report: \n", class_svm_2)

"""

**5. K-Nearest Neighbors**"""

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_1, Y_train_1)
prediction_knn_1 = knn.predict(X_test_1)
print('\n********** K-Nearest Neighbors **********\n')
acc_knn_1 = accuracy_score(Y_test_1, prediction_knn_1)
con_knn_1 = confusion_matrix(Y_test_1, prediction_knn_1)
class_knn_1 = classification_report(Y_test_1, prediction_knn_1)
print("Accuracy Score: \n", acc_knn_1)
print("Confusion Matrix: \n", con_knn_1)
print("Classification Report: \n", class_knn_1)

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_2, Y_train_2)
prediction_knn_2 = knn.predict(X_test_2)
print('\n********** K-Nearest Neighbors **********\n')
acc_knn_2 = accuracy_score(Y_test_2, prediction_knn_2)
con_knn_2 = confusion_matrix(Y_test_2, prediction_knn_2)
class_knn_2 = classification_report(Y_test_2, prediction_knn_2)
print("Accuracy Score: \n", acc_knn_2)
print("Confusion Matrix: \n", con_knn_2)
print("Classification Report: \n", class_knn_2)

"""**6. Gaussian Naive Bayes**"""

gnb = GaussianNB()
gnb.fit(X_train_1, Y_train_1)
prediction_gnb_1 = gnb.predict(X_test_1)
print('\n********** Gaussian Naive Bayes **********\n')
acc_gnb_1 = accuracy_score(Y_test_1, prediction_gnb_1)
con_gnb_1 = confusion_matrix(Y_test_1, prediction_gnb_1)
class_gnb_1 = classification_report(Y_test_1, prediction_gnb_1)
print("Accuracy Score: \n", acc_gnb_1)
print("Confusion Matrix: \n", con_gnb_1)
print("Classification Report: \n", class_gnb_1)

gnb = GaussianNB()
gnb.fit(X_train_2, Y_train_2)
prediction_gnb_2 = gnb.predict(X_test_2)
print('\n********** Gaussian Naive Bayes **********\n')
acc_gnb_2 = accuracy_score(Y_test_2, prediction_gnb_2)
con_gnb_2 = confusion_matrix(Y_test_2, prediction_gnb_2)
class_gnb_2 = classification_report(Y_test_2, prediction_gnb_2)
print("Accuracy Score: \n", acc_gnb_2)
print("Confusion Matrix: \n", con_gnb_2)
print("Classification Report: \n", class_gnb_2)

"""We are able to see that the accuracy score and f1-score for all the algorithm, is high in Income_dataset2 compared to Income_dataset1.

So, the mean/mode method for handling the missing value is feasible.

Among these models **Random Forest Classifier** gives the highest accuracy(almost 85%) with Income_dataset2. So, we will now work only with the income_dataset2.

# Correlation of Columns:

The coefficient returns a value between -1 and 1 that represents the limits of correlation from a full negative correlation to a full positive correlation. A value of 0 means no correlation.

- **Positive Correlation:** both variables change in the same direction, meaning that when one variable’s value increases, the other variable's values also increases.
- **Neutral Correlation:**  No relationship in the change of the variables.
- **Negative Correlation:** variables change in opposite directions, meaning that when one variable’s value increases, the other variable's values decreases.

The most commonly used is the **Pearson Correlation** method.

Mathematically, if (σXY) is the covariance between X and Y, and (σX) is the standard deviation of X, (σY) is the standard deviation of Y, then the Pearson's correlation coefficient ρ is given by:

ρ(X,Y) = σ(XY) / (σX)(σY)
"""

correlation2 = income_dataset2.corr(method='pearson')

# constructing a heatmap to understand the correlation
plt.figure(figsize=(15, 15))
sns.heatmap(correlation2, cbar=True, square=True, annot=True, fmt='.3f', annot_kws={'size':8}, cmap='Greens')

"""# Feature Selection:"""

# drop the unnecessary column from the Income dataset
income_dataset2 = income_dataset2.drop(columns=['fnlwgt', 'workclass', 'education', 'race'], axis=1)

income_dataset2.head()

X_2 = income_dataset2.iloc[:,:-1]
Y_2 = income_dataset2['income_>50K']

print(X_2)

print(Y_2)

X_train_2, X_test_2, Y_train_2, Y_test_2 = train_test_split(X_2, Y_2, test_size = 0.2, random_state = 30, stratify = Y_2)

print(X_train_2.shape, X_test_2.shape, Y_train_2.shape, Y_test_2.shape)

scaler = StandardScaler()

X_train_2 = pd.DataFrame(scaler.fit_transform(X_train_2), columns=X_train_2.columns)
X_test_2 = pd.DataFrame(scaler.fit_transform(X_test_2), columns=X_test_2.columns)

X_train_2.head()

print(X_train_2.shape, X_test_2.shape, Y_train_2.shape, Y_test_2.shape)

"""# Training the data's with different models(With Hyperparameter tunning):

**1.Random Forest Classifier**
"""

print("\n********** Random Forest Classifier ***********\n")

print("\n********** Using Hyperparameter Tuning  ***********\n")

RF = RandomForestClassifier()
param_grid = {
    # Number of trees in random forest
    'n_estimators': [10, 50, 100],
    # Maximum depth of tree
    'max_depth': [None, 5, 10],
    # minimum number of samples needed to split
    'min_samples_split': [ 2, 5, 10],
    # Number of features to consider at every split
    'max_features' : ['log2', 'sqrt'],
    # Method of selecting samples for training each tree
    'bootstrap' : [True, False],
    'criterion' : ['gini', 'entropy']
}


# Random search of parameters, using 10 fold cross validation,
RFC = GridSearchCV(RF, param_grid, scoring='f1', cv=10)
RFC.fit(X_train_2, Y_train_2)
print("Best score: ", RFC.best_score_)
prediction_RFC = RFC.predict(X_test_2)
print("Accuracy Score:\n",accuracy_score(Y_test_2, prediction_RFC))
print("Confusion Matrix:\n", confusion_matrix(Y_test_2,prediction_RFC))
print("Classification Report:\n", classification_report(Y_test_2,prediction_RFC))
print("\n*********************************")
print("Best hyperparameters:", RFC.best_params_)

"""**2. Logistic Regression**"""

print("\n********** Logistic Regression ***********\n")

print("\n********** Using Hyperparameter Tuning ***********\n")

params = {
    # Specifies the type of regularization(used for reducing overfitting of model) used
    'penalty': ['l1', 'l2'], 
    # Inverse of regularization
    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
    # For solving the optimization
    'solver': ['liblinear', 'saga'],
    'max_iter' : [100, 500, 1000]
}

LR = LogisticRegression()

grid_search_LR = GridSearchCV(LR, params, scoring='f1', cv=10)
grid_search_LR.fit(X_train_2, Y_train_2)
best_params = grid_search_LR.best_params_

# Train logistic regression model with best hyperparameters
LR = LogisticRegression(**best_params)
LR.fit(X_train_2, Y_train_2)
prediction_LR = LR.predict(X_test_2)
print("Accuracy Score:\n", accuracy_score(Y_test_2,prediction_LR))
print("Confusion Matrix:\n", confusion_matrix(Y_test_2,prediction_LR))
print("Classification Report:\n", classification_report(Y_test_2,prediction_LR))
print("\n*********************************")
print("Best hyperparameters:\n", best_params)
print("Best score: ", grid_search_LR.best_score_)

"""**3. Decision Tree Classifier**"""

print("\n********** Decision Tree Classifier ***********\n")

print("\n********** Using Hyperparameter Tuning ***********\n")
param_grid = {
    'criterion': ['gini', 'entropy'],
    # maximum depth  of tree
    'max_depth': [10, 20, 30, 50],
    # minimum number of samples needed to split
    'min_samples_split': [2, 4, 6, 8, 10],
    'min_samples_leaf': [1, 2, 3, 4, 5]
}

DT = DecisionTreeClassifier(random_state = 40)

# Perform grid search using 10-fold cross validation
grid_search_DT = GridSearchCV(DT, param_grid, scoring='f1', cv=10)
grid_search_DT.fit(X_train_2, Y_train_2)
prediction_DT=grid_search_DT.predict(X_test_2)
print("Accuracy Score:\n", accuracy_score(Y_test_2,prediction_DT))
print("Confusion Matrix:\n", confusion_matrix(Y_test_2,prediction_DT))
print("Classification Report:\n", classification_report(Y_test_2,prediction_DT))
print("\n*********************************")
print("Best Hyperparameters:", grid_search_DT.best_params_)
print("Best score: ", grid_search_DT.best_score_)

"""**4. K-Nearest Neighbors**"""

print("\n********** K-Nearest Neighbors ***********\n")
print("\n********** Using Hyperparameter Tuning ***********\n")
KNN = KNeighborsClassifier()

param_grid = {'n_neighbors' : [3, 5, 7, 9],         # number of nearest neighbour
              'weights' : ['uniform', 'distance'],           # assign weight to neighbour
              'metric' : ['euclidean','manhattan','minkowski']      # specification of distance
}

# Perform the grid search using 10 fold cross validations
grid_search_KNN =  GridSearchCV(KNN, param_grid, scoring='f1', cv=10)
grid_search_KNN.fit(X_train_2, Y_train_2)
prediction_KNN=grid_search_KNN.predict(X_test_2)
print("Accuracy Score:\n", accuracy_score(Y_test_2, prediction_KNN))
print("Confusion Matrix:\n", confusion_matrix(Y_test_2, prediction_KNN))
print("Classification Report:\n", classification_report(Y_test_2, prediction_KNN))
print("\n*********************************")
print("Best Hyperparameters:", grid_search_KNN.best_params_)
print("Best score: ", grid_search_KNN.best_score_)

"""**5. Gaussian Naive Bayes**"""

print("\n********** Gaussian Naive Bayes ***********\n")
print("\n********** Using Hyperparameter Tuning ***********\n")

param_grid = {
    'var_smoothing': [1e-09, 1e-08, 1e-07, 1e-06, 1e-05]      # smoothing of variants. Specify the amount of variance to add to features.
}
GNB = GaussianNB()

# Perform grid search using 10-fold cross validation
grid_search_GNB = GridSearchCV(GNB, param_grid = param_grid, scoring='f1', cv = 10)
grid_search_GNB.fit(X_train_2, Y_train_2)
prediction_GNB = grid_search_GNB.predict(X_test_2)
print("Accuracy Score:\n", accuracy_score(Y_test_2, prediction_GNB))
print("Confusion Matrix:\n", confusion_matrix(Y_test_2, prediction_GNB))
print("Classification Report:\n", classification_report(Y_test_2, prediction_GNB))
print("\n*********************************")
print("Best Hyperparameters: ", grid_search_GNB.best_params_)
print("Best score: ", grid_search_GNB.best_score_)

"""# Testing the model with test dataset provided:

Now we are finding the target outcome for the given test dataset.
"""

test = pd.read_csv('/content/test.csv')

# printing the first 5 rows of the dataframe
test.head()

# number of rows and columns
test.shape

# number of missing values in each column
test.isnull().sum()

encoder = LabelEncoder()

test['workclass'] = encoder.fit_transform(test['workclass'])

test['education'] = encoder.fit_transform(test['education'])

test['marital-status'] = encoder.fit_transform(test['marital-status'])

test['occupation'] = encoder.fit_transform(test['occupation'])

test['relationship'] = encoder.fit_transform(test['relationship'])

test['race'] = encoder.fit_transform(test['race'])

test['gender'] = encoder.fit_transform(test['gender'])

test['native-country'] = encoder.fit_transform(test['native-country'])

test.head()

scaler = StandardScaler()

X_test = pd.DataFrame(scaler.fit_transform(test), columns=test.columns)

# drop the unnecessary column from the test dataset
X_test = X_test.drop(columns=['fnlwgt', 'workclass', 'education', 'race'], axis=1)

X_test.head()

X_test.shape

RFC_test = RandomForestClassifier(criterion='gini', max_features='sqrt', min_samples_split=10, 
                             n_estimators=100, bootstrap=True, max_depth=None)

RFC_test.fit(X_train_2, Y_train_2)
prediction_RFC = RFC_test.predict(X_test)
print("\n*********************************")
print(prediction_RFC)
print("\n*********************************")
print("Shape of the class labels ", prediction_RFC.shape)

df_pred = pd.DataFrame(prediction_RFC, columns=['Pred_label'])
df_pred

df_pred.to_csv('Test_pred_labels.csv')